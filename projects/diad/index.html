<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="DiAD: A Diffusion-based Framework for Multi-class Anomaly Detection">
  <meta name="keywords" content="Diffusion model, Multi-class, Anomaly Detection">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DiAD: A Diffusion-based Framework for Multi-class Anomaly Detection</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!--<nav class="navbar" role="navigation" aria-label="main navigation">-->
<!--  <div class="navbar-brand">-->
<!--    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">-->
<!--      <span aria-hidden="true"></span>-->
<!--      <span aria-hidden="true"></span>-->
<!--      <span aria-hidden="true"></span>-->
<!--    </a>-->
<!--  </div>-->
<!--  <div class="navbar-menu">-->
<!--    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">-->
<!--      <a class="navbar-item" href="https://keunhong.com">-->
<!--      <span class="icon">-->
<!--          <i class="fas fa-home"></i>-->
<!--      </span>-->
<!--      </a>-->

<!--      <div class="navbar-item has-dropdown is-hoverable">-->
<!--        <a class="navbar-link">-->
<!--          More Research-->
<!--        </a>-->
<!--        <div class="navbar-dropdown">-->
<!--          <a class="navbar-item" href="https://hypernerf.github.io">-->
<!--            HyperNeRF-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://nerfies.github.io">-->
<!--            Nerfies-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://latentfusion.github.io">-->
<!--            LatentFusion-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://photoshape.github.io">-->
<!--            PhotoShape-->
<!--          </a>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->

<!--  </div>-->
<!--</nav>-->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">DiAD: A Diffusion-based Framework for Multi-class Anomaly Detection</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=8NfQv1sAAAAJ">Haoyang He</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://zhangzjn.github.io">Jiangning Zhang</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="">Hongxu Chen</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=LU4etJ0AAAAJ">Xuhai Chen</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=9g-IRLsAAAAJ">Zhishan Li</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=1621dVIAAAAJ">Xu Chen</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=xiK4nFUAAAAJ">Yabiao Wang</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=fqte5H4AAAAJ">Chengjie Wang</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=7ZZ_-m0AAAAJ">Lei Xie</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Zhejiang University</span>
            <span class="author-block"><sup>2</sup>Youtu Lab, Tencent</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
<!--              </span>-->
<!--              &lt;!&ndash; Video Link. &ndash;&gt;-->
<!--              <span class="link-block">-->
<!--                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fab fa-youtube"></i>-->
<!--                  </span>-->
<!--                  <span>Video</span>-->
<!--                </a>-->
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/lewandofskee/DiAD"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
<!--              </span>-->
<!--              &lt;!&ndash; Dataset Link. &ndash;&gt;-->
<!--              <span class="link-block">-->
<!--                <a href="https://github.com/google/nerfies/releases/tag/0.1"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="far fa-images"></i>-->
<!--                  </span>-->
<!--                  <span>Data</span>-->
<!--                  </a>-->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!--<section class="hero teaser">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="hero-body">-->
<!--      <video id="teaser" autoplay muted loop playsinline height="100%">-->
<!--        <source src="./static/videos/teaser.mp4"-->
<!--                type="video/mp4">-->
<!--      </video>-->
<!--      <h2 class="subtitle has-text-centered">-->
<!--        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into-->
<!--        free-viewpoint-->
<!--        portraits.-->
<!--      </h2>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Reconstruction-based approaches have achieved remarkable outcomes in anomaly detection.
            The exceptional image reconstruction capabilities of recently popular diffusion models have sparked
            research efforts to utilize them for enhanced reconstruction of anomalous images.
            Nonetheless, these methods might face challenges related to the preservation of image categories and
            pixel-wise structural integrity in the more
            practical multi-class setting. To solve the above problems,
            we propose a Difusion-based Anomaly Detection (DiAD)
            framework for multi-class anomaly detection, which consists of a pixel-space autoencoder,
            a latent-space Semantic-Guided (SG) network with a connection to the stable diffusion’s denoising network,
            and a feature-space pre-trained feature extractor. Firstly, The SG network is proposed for
            reconstructing anomalous regions while preserving the original image’s semantic information. Secondly, we introduce
            Spatial-aware Feature Fusion (SFF) block to maximize reconstruction accuracy when dealing with extensively
            reconstructed areas. Thirdly, the input and reconstructed images
            are processed by a pre-trained feature extractor to generate
            anomaly maps based on features extracted at different scales.
            Experiments on MVTec-AD and VisA datasets demonstrate
            the effectiveness of our approach which surpasses the state-of-the-art methods, e.g., achieving 96.8/52.6 and 97.2/99.0
            (AUROC/AP) for localization and detection respectively on
            multi-class MVTec-AD dataset.
          </p>
        </div>
      </div>

    </div>
    <div class="column is-center has-text-centered">
            <img src="./static/images/diad_wt.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
      </div>
    <!--/ Abstract. -->

<!--    &lt;!&ndash; Paper video. &ndash;&gt;-->
<!--    <div class="columns is-centered has-text-centered">-->
<!--      <div class="column is-four-fifths">-->
<!--        <h2 class="title is-3">Video</h2>-->
<!--        <div class="publication-video">-->
<!--          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"-->
<!--                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--    &lt;!&ndash;/ Paper video. &ndash;&gt;-->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

<!--      &lt;!&ndash; Visual Effects. &ndash;&gt;-->
<!--      <div class="column">-->
<!--        <div class="content">-->
<!--          <h2 class="title is-3">Visual Effects</h2>-->
<!--          <p>-->
<!--            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect-->
<!--            would be impossible without nerfies since it would require going through a wall.-->
<!--          </p>-->
<!--          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/dollyzoom-stacked.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--      </div>-->
<!--      &lt;!&ndash;/ Visual Effects. &ndash;&gt;-->

<!--      &lt;!&ndash; Matting. &ndash;&gt;-->
<!--      <div class="column">-->
<!--        <h2 class="title is-3">Matting</h2>-->
<!--        <div class="columns is-centered">-->
<!--          <div class="column content">-->
<!--            <p>-->
<!--              As a byproduct of our method, we can also solve the matting problem by ignoring-->
<!--              samples that fall outside of a bounding box during rendering.-->
<!--            </p>-->
<!--            <video id="matting-video" controls playsinline height="100%">-->
<!--              <source src="./static/videos/matting.mp4"-->
<!--                      type="video/mp4">-->
<!--            </video>-->
<!--          </div>-->

<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--    &lt;!&ndash;/ Matting. &ndash;&gt;-->

    <!-- Qualitative Results. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Qualitative Results</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">MVTec-AD Dataset</h3>
        <div class="content has-text-justified">
          <p>
            We conducted substantial qualitative
            experiments on MVTec-AD and VisA datasets to visually
            demonstrate the superiority of our method in image reconstruction
            and the accuracy of anomaly localization. As
            shown in Figure 4, our method exhibits better reconstruction
            capabilities for anomalous regions compared to the EdgRec
            on MVTec-AD dataset. In comparison to UniAD shown in
            Figure 5, our method exhibits more accurate anomaly localization
            abilities on VisA dataset.
          </p>
        </div>
        <div class="column is-center has-text-centered">
            <img src="./static/images/appdendix_mvtec_show.png.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
        </div>
        <div class="column is-center has-text-centered">
            <img src="./static/images/appdendix_mvtec_show2.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
        </div>

        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">VisA Dataset</h3>

        <div class="column is-center has-text-centered">
            <img src="./static/images/app_visa_show.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
        </div>

      </div>
    </div>
    <!--/ Animation. -->

    <!-- Quantitative Results. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Quantitative Results</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">MVTec-AD Dataset</h3>
        <div class="content has-text-justified">
          <p>
            As shown in Table 1 and in Table 3, our method achieves SOTA AUROC/AP/F1max metrics of 97.2/99.0/96.5 and 96.8/52.6/55.5 for image-wise and
            pixel-wise respectively for multi-class setting on MVTec-
            AD dataset. For the diffusion-based methods, our approach
            significantly outperforms existing DDPM and LDM methods in terms of 11.7↑ in AUROC and 25↑ in AP for anomaly
            localization. For non-diffusion methods, our approach surpasses existing methods in both metrics, especially at the
            pixel level, where our method exceeds UniAD by 9.2↑/6.0↑
            in AP/F1max.
          </p>
        </div>
        <div class="column is-center has-text-centered">
            <img src="./static/images/cls.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
        </div>
        <div class="column is-center has-text-centered">
            <img src="./static/images/seg.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
        </div>

        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">VisA Dataset</h3>
        <div class="content has-text-justified">
          <p>
            Our method has also demonstrated its superiority on VisA dataset, as shown in Table 2. Our approach
            exhibits significant improvements compared to diffusion-based methods of 30.1↑/9.4↑ than the LDM method in
            image/pixel AUROC. It also performs well compared to
            UniAD by 4.9↑/6.0↑ in pixel AP/F1max metrics. Detailed
            experiments for each category are provided in Appendix.
          </p>
        </div>
        <div class="column is-center has-text-centered">
            <img src="./static/images/visa.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
        </div>

      </div>
    </div>
    <!--/ Quantitative Results. -->


<!--    &lt;!&ndash; Concurrent Work. &ndash;&gt;-->
<!--    <div class="columns is-centered">-->
<!--      <div class="column is-full-width">-->
<!--        <h2 class="title is-3">Related Links</h2>-->

<!--        <div class="content has-text-justified">-->
<!--          <p>-->
<!--            There's a lot of excellent work that was introduced around the same time as ours.-->
<!--          </p>-->
<!--          <p>-->
<!--            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.-->
<!--          </p>-->
<!--          <p>-->
<!--            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>-->
<!--            both use deformation fields to model non-rigid scenes.-->
<!--          </p>-->
<!--          <p>-->
<!--            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>-->
<!--          </p>-->
<!--          <p>-->
<!--            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.-->
<!--          </p>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--    &lt;!&ndash;/ Concurrent Work. &ndash;&gt;-->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            We borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> for our website.
            We sincerely appreciate Nerfies authors for their awesome templates.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
